import pandas as pd
import json
import concurrent.futures
from core.translate_lines import translate_lines
from core._4_1_summarize import search_things_to_note_in_prompt
from core._8_1_audio_task import check_len_then_trim
from core._6_gen_sub import align_timestamp
from core.utils import *
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from difflib import SequenceMatcher
from core.utils.models import *
console = Console()

# Function to split text into chunks
def split_chunks_by_chars(chunk_size, max_i): 
    """Split text into chunks based on character count, return a list of multi-line text chunks"""
    with open(_3_2_SPLIT_BY_MEANING, "r", encoding="utf-8") as file:
        sentences = file.read().strip().split('\n')

    chunks = []
    chunk = ''
    sentence_count = 0
    for sentence in sentences:
        if len(chunk) + len(sentence + '\n') > chunk_size or sentence_count == max_i:
            chunks.append(chunk.strip())
            chunk = sentence + '\n'
            sentence_count = 1
        else:
            chunk += sentence + '\n'
            sentence_count += 1
    chunks.append(chunk.strip())
    return chunks

# Get context from surrounding chunks
def get_previous_content(chunks, chunk_index):
    return None if chunk_index == 0 else chunks[chunk_index - 1].split('\n')[-3:] # Get last 3 lines
def get_after_content(chunks, chunk_index):
    return None if chunk_index == len(chunks) - 1 else chunks[chunk_index + 1].split('\n')[:2] # Get first 2 lines

# ðŸ” Translate a single chunk
def translate_chunk(chunk, chunks, theme_prompt, i):
    things_to_note_prompt = search_things_to_note_in_prompt(chunk)
    previous_content_prompt = get_previous_content(chunks, i)
    after_content_prompt = get_after_content(chunks, i)
    translation, english_result = translate_lines(chunk, previous_content_prompt, after_content_prompt, things_to_note_prompt, theme_prompt, i)
    return i, english_result, translation

# Add similarity calculation function
def similar(a, b):
    return SequenceMatcher(None, a, b).ratio()

# ðŸš€ Main function to translate all chunks
@check_file_exists(_4_2_TRANSLATION)
def translate_all():
    console.print("[bold green]Start Translating All...[/bold green]")
    chunks = split_chunks_by_chars(chunk_size=600, max_i=15)
    
    # ------------
    # Batch translate to reduce LLM calls
    # ------------
    try:
        batch_size = load_key("batch_translate_size")
    except:
        batch_size = 15  # Default batch size
    # Process chunks per LLM call to avoid rate limiting
    with open(_4_1_TERMINOLOGY, 'r', encoding='utf-8') as file:
        theme_prompt = json.load(file).get('theme')

    # ðŸ”„ Process chunks in batches to reduce LLM calls
    all_results = {}
    
    with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), transient=True) as progress:
        total_batches = (len(chunks) + batch_size - 1) // batch_size
        task = progress.add_task(f"[cyan]Processing {total_batches} batches...", total=total_batches)
        
        for batch_start in range(0, len(chunks), batch_size):
            batch_end = min(batch_start + batch_size, len(chunks))
            batch_chunks = chunks[batch_start:batch_end]
            
            # Prepare batch data
            chunks_data = []
            for i, chunk in enumerate(batch_chunks):
                chunk_index = batch_start + i
                chunks_data.append({
                    'chunk': chunk,
                    'previous_content': get_previous_content(chunks, chunk_index),
                    'after_content': get_after_content(chunks, chunk_index),
                    'things_to_note': search_things_to_note_in_prompt(chunk),
                    'index': chunk_index
                })
            
            # Import batch translation function
            from core.translate_lines import translate_lines_batch
            
            # Batch translate
            console.print(f"[cyan]ðŸ”„ Processing batch {batch_start//batch_size + 1}/{total_batches} ({len(batch_chunks)} chunks)...[/cyan]")
            batch_results = translate_lines_batch(chunks_data, theme_prompt)
            
            # Merge results
            all_results.update(batch_results)
            progress.update(task, advance=1)

    # ðŸ’¾ Save results to lists and Excel file
    src_text, trans_text = [], []
    for i, chunk in enumerate(chunks):
        chunk_lines = chunk.split('\n')
        src_text.extend(chunk_lines)
        
        if i in all_results:
            trans_text.extend(all_results[i]['translation'].split('\n'))
        else:
            console.print(f"[red]âŒ Missing translation for chunk {i}[/red]")
            raise ValueError(f"Translation failed for chunk {i}")
    
    # Trim long translation text
    df_text = pd.read_excel(_2_CLEANED_CHUNKS)
    df_text['text'] = df_text['text'].str.strip('"').str.strip()
    df_translate = pd.DataFrame({'Source': src_text, 'Translation': trans_text})
    subtitle_output_configs = [('trans_subs_for_audio.srt', ['Translation'])]
    df_time = align_timestamp(df_text, df_translate, subtitle_output_configs, output_dir=None, for_display=False)
    console.print(df_time)
    # apply check_len_then_trim to df_time['Translation'], only when duration > MIN_TRIM_DURATION.
    df_time['Translation'] = df_time.apply(lambda x: check_len_then_trim(x['Translation'], x['duration']) if x['duration'] > load_key("min_trim_duration") else x['Translation'], axis=1)
    console.print(df_time)
    
    df_time.to_excel(_4_2_TRANSLATION, index=False)
    console.print("[bold green]âœ… Translation completed and results saved.[/bold green]")

if __name__ == '__main__':
    translate_all()